{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9508f8ae-501a-4315-85ac-82cedee25f6f",
   "metadata": {},
   "source": [
    "# Apache Spark Group Assignment - Multi Feature Bitcoin Price and Direction Forecasting \n",
    "This notebook is organized in the following sections:\n",
    "* [Description](#0)\n",
    "* [Step 1 - Historical Data Collection](#2)\n",
    "* [Step 2 - Initializing Spark Session and defining Features and Label](#3)\n",
    "* [Step 3 - Raw Multi Model Traininig/Evaluation](#4)\n",
    "* [Step 4 - Multi Model Traininig/Evaluation With Feature Engineering/Selection and Imbalance with SMOTE](#5)\n",
    "* [Step 5 - Simulated Streaming Data Forecasting with MicroBatching](#6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191176ba-2a92-485f-8d83-cc318a91ea35",
   "metadata": {},
   "source": [
    "## **Description**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b029d5-5b45-4f55-b462-c468a4996a47",
   "metadata": {},
   "source": [
    "The objective of this project is to forecast the Bitcoin price and its direction, using a Linear Regression and a Logistic Regression Model.\n",
    "The features used are the prices and funding rates of the following top 5 Altcoins by marketcap.\n",
    "\n",
    "The data pipeline starts with an API request to 2 different binance endpoints as we're looking both for prices and funding rates, this are stored as csv's in HDFS and subsequently fetched from there to convert them to spark DataFrames.\n",
    "\n",
    "The next step consists on splitting the DF for training and testing, performing feature scaling and selection to feed and train the Linear and Logistic regression models.\n",
    "\n",
    "Finally we emulate Streaming data via MicroBatching to predict in real time the price and direction of Bitcoin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c6340f-3cab-4398-bd05-ec269896683c",
   "metadata": {},
   "source": [
    "## **Step 1 - Historical Data collection**\n",
    "\n",
    "In this section, we'll focus on fetching historical data both for token prices and their funding rate, as the combined DataFrame will be instrumental to fit and train our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613e1ae2-f622-40a7-9b72-3491be747ad8",
   "metadata": {},
   "source": [
    "### Imports and Constants\n",
    "Imports libraries for API requests (`requests`), data manipulation (`pandas`), time handling (`time`, `datetime`), and defines constants for Binance API endpoints and cryptocurrency symbols to fetch data for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daf44659-dc18-47e7-befb-de0a328ae8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "BASE_URL = \"https://api.binance.com\"  # Spot API for prices\n",
    "FUTURES_URL = \"https://fapi.binance.com\"  # Futures API for funding rates\n",
    "\n",
    "SYMBOLS = [\"BTCUSDT\", \"ETHUSDT\", \"BNBUSDT\", \"XRPUSDT\", \"ADAUSDT\", \"SOLUSDT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f4d302-eab5-430a-b976-7e3aed0c44c5",
   "metadata": {},
   "source": [
    "### Fetch Binance Klines Function\n",
    "Defines a function to fetch historical price data (klines) from Binance Spot API for a given symbol. Retrieves 8-hour interval data over the past year, paginating through results, and returns a DataFrame with timestamps and closing prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbb84123-f8e8-432c-a10d-53c9908606e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_binance_klines(symbol, interval=\"8h\", days=365):\n",
    "    url = f\"{BASE_URL}/api/v3/klines\"\n",
    "    end_time = int(time.time() * 1000)  # Current time in ms\n",
    "    start_time = end_time - (days * 24 * 60 * 60 * 1000)  # 1 year ago in ms\n",
    "    \n",
    "    params = {\n",
    "        \"symbol\": symbol,\n",
    "        \"interval\": interval,\n",
    "        \"startTime\": start_time,\n",
    "        \"endTime\": end_time,\n",
    "        \"limit\": 1000  # Max limit per request\n",
    "    }\n",
    "    \n",
    "    all_data = []\n",
    "    while start_time < end_time:\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if not data:\n",
    "                break\n",
    "            all_data.extend(data)\n",
    "            start_time = int(data[-1][0]) + 1  # Next batch\n",
    "            params[\"startTime\"] = start_time\n",
    "            time.sleep(0.5)  # Avoid rate limits\n",
    "        else:\n",
    "            print(f\"Error fetching {symbol} klines: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "    \n",
    "    df = pd.DataFrame(all_data, columns=[\"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\", \n",
    "                                         \"close_time\", \"quote_volume\", \"trades\", \"taker_base\", \n",
    "                                         \"taker_quote\", \"ignore\"])\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\")\n",
    "    df[\"close\"] = df[\"close\"].astype(float)\n",
    "    return df[[\"timestamp\", \"close\"]].rename(columns={\"close\": f\"{symbol}_price\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662e9882-9aa3-4b45-931e-64c167c8aff0",
   "metadata": {},
   "source": [
    "### Fetch Binance Funding Rate Function\n",
    "Defines a function to fetch historical funding rates from Binance Futures API for a given symbol. Collects data over the past year, paginating through results, and returns a DataFrame with timestamps and funding rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f9bbc0-b1ab-4344-b135-c6da11e9ee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_binance_funding_rate(symbol, days=365):\n",
    "    \"\"\"Fetch historical funding rates from Binance Futures.\"\"\"\n",
    "    url = f\"{FUTURES_URL}/fapi/v1/fundingRate\"\n",
    "    end_time = int(time.time() * 1000)\n",
    "    start_time = end_time - (days * 24 * 60 * 60 * 1000)\n",
    "    \n",
    "    params = {\n",
    "        \"symbol\": symbol,\n",
    "        \"startTime\": start_time,\n",
    "        \"endTime\": end_time,\n",
    "        \"limit\": 1000\n",
    "    }\n",
    "    \n",
    "    all_data = []\n",
    "    while start_time < end_time:\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if not data:\n",
    "                break\n",
    "            all_data.extend(data)\n",
    "            start_time = int(data[-1][\"fundingTime\"]) + 1\n",
    "            params[\"startTime\"] = start_time\n",
    "            time.sleep(0.5)\n",
    "        else:\n",
    "            print(f\"Error fetching {symbol} funding: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "    \n",
    "    # Convert to DataFrame (timestamp, funding rate)\n",
    "    df = pd.DataFrame(all_data)\n",
    "    df[\"fundingTime\"] = pd.to_datetime(df[\"fundingTime\"], unit=\"ms\")\n",
    "    df[\"fundingRate\"] = df[\"fundingRate\"].astype(float)\n",
    "    return df[[\"fundingTime\", \"fundingRate\"]].rename(columns={\"fundingTime\": \"timestamp\", \"fundingRate\": f\"{symbol}_funding_rate\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05be9bb-16d8-41ed-9d10-acaea7e86c54",
   "metadata": {},
   "source": [
    "### Fetch Data for All Symbols\n",
    "Loops through the list of symbols, fetching price data and funding rates for each using the defined functions. Stores the resulting DataFrames in separate lists for prices and funding rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88108e8a-415a-4564-842d-02f51d5ea0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for BTCUSDT...\n",
      "Fetching data for ETHUSDT...\n",
      "Fetching data for BNBUSDT...\n",
      "Fetching data for XRPUSDT...\n",
      "Fetching data for ADAUSDT...\n",
      "Fetching data for SOLUSDT...\n"
     ]
    }
   ],
   "source": [
    "# Fetch and combine data\n",
    "price_dfs = []\n",
    "funding_dfs = []\n",
    "for symbol in SYMBOLS:\n",
    "    print(f\"Fetching data for {symbol}...\")\n",
    "    price_df = fetch_binance_klines(symbol)\n",
    "    funding_df = fetch_binance_funding_rate(symbol)\n",
    "    price_dfs.append(price_df)\n",
    "    funding_dfs.append(funding_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ed8e4d-55ba-458c-8590-5b06653bf0f5",
   "metadata": {},
   "source": [
    "### Merge Price Data\n",
    "Merges all price DataFrames into a single DataFrame, aligning them by `timestamp` using an outer join to preserve all data points across symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96022d84-72e4-4cc6-a618-2b3f18c1002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge price data\n",
    "combined_price = price_dfs[0]\n",
    "for df in price_dfs[1:]:\n",
    "    combined_price = combined_price.merge(df, on=\"timestamp\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c1d089-8517-41a9-87c6-326f0a822810",
   "metadata": {},
   "source": [
    "### Merge Funding Rate Data\n",
    "Merges all funding rate DataFrames into a single DataFrame, aligning them by `timestamp` using an outer join to preserve all funding rate data across symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b660da4-b574-4d43-bae5-3d31155b81e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge funding rate data\n",
    "combined_funding = funding_dfs[0]\n",
    "for df in funding_dfs[1:]:\n",
    "    combined_funding = combined_funding.merge(df, on=\"timestamp\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e214f88-409e-41cd-962d-44e98b1d88a2",
   "metadata": {},
   "source": [
    "### Combine Prices and Funding Rates\n",
    "Merges the combined price and funding rate DataFrames on `timestamp` using an inner join to ensure only rows with matching timestamps are kept. Drops any remaining rows with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f7e4282-f2f0-4b42-8634-66c7896ae41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge prices and funding rates (align timestamps)\n",
    "data = combined_price.merge(combined_funding, on=\"timestamp\", how=\"inner\")\n",
    "data = data.dropna()  # Drop rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6a1fd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              timestamp  BTCUSDT_price  ETHUSDT_price  BNBUSDT_price  \\\n",
      "0   2024-05-27 16:00:00       69436.43        3894.22         603.90   \n",
      "1   2024-05-28 00:00:00       67694.00        3847.29         597.80   \n",
      "2   2024-05-28 08:00:00       68374.08        3861.10         601.40   \n",
      "3   2024-05-28 16:00:00       68398.39        3844.69         601.70   \n",
      "4   2024-05-29 08:00:00       67379.13        3759.85         595.40   \n",
      "..                  ...            ...            ...            ...   \n",
      "866 2025-05-24 08:00:00      108929.70        2555.64         674.09   \n",
      "867 2025-05-25 08:00:00      106967.97        2511.70         663.20   \n",
      "868 2025-05-25 16:00:00      109004.19        2551.22         669.44   \n",
      "869 2025-05-26 00:00:00      110094.62        2580.80         674.74   \n",
      "870 2025-05-26 08:00:00      110009.82        2565.99         675.41   \n",
      "\n",
      "     XRPUSDT_price  ADAUSDT_price  SOLUSDT_price  BTCUSDT_funding_rate  \\\n",
      "0           0.5340         0.4683         170.15              0.000100   \n",
      "1           0.5264         0.4555         166.09              0.000100   \n",
      "2           0.5301         0.4579         168.28              0.000100   \n",
      "3           0.5287         0.4576         168.68              0.000100   \n",
      "4           0.5280         0.4556         168.60              0.000100   \n",
      "..             ...            ...            ...                   ...   \n",
      "866         2.3523         0.7584         176.31              0.000046   \n",
      "867         2.2877         0.7400         170.29              0.000051   \n",
      "868         2.3417         0.7598         175.74              0.000037   \n",
      "869         2.3488         0.7753         178.16              0.000073   \n",
      "870         2.3328         0.7686         177.49              0.000100   \n",
      "\n",
      "     ETHUSDT_funding_rate  BNBUSDT_funding_rate  XRPUSDT_funding_rate  \\\n",
      "0                0.000100              0.000198              0.000119   \n",
      "1                0.000100              0.000178              0.000100   \n",
      "2                0.000100              0.000000              0.000100   \n",
      "3                0.000122              0.000000              0.000205   \n",
      "4                0.000100              0.000060              0.000137   \n",
      "..                    ...                   ...                   ...   \n",
      "866              0.000040              0.000000              0.000023   \n",
      "867              0.000037              0.000000             -0.000059   \n",
      "868              0.000085              0.000000             -0.000063   \n",
      "869              0.000004              0.000000              0.000049   \n",
      "870              0.000080              0.000000              0.000100   \n",
      "\n",
      "     ADAUSDT_funding_rate  SOLUSDT_funding_rate  \n",
      "0                0.000100              0.000100  \n",
      "1                0.000114              0.000100  \n",
      "2                0.000100              0.000100  \n",
      "3                0.000100              0.000141  \n",
      "4                0.000100              0.000137  \n",
      "..                    ...                   ...  \n",
      "866             -0.000038             -0.000021  \n",
      "867              0.000005             -0.000038  \n",
      "868              0.000016             -0.000037  \n",
      "869              0.000018             -0.000010  \n",
      "870              0.000002              0.000018  \n",
      "\n",
      "[871 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73b5d17-9a26-4b40-9e9d-8ee5de1dbfa3",
   "metadata": {},
   "source": [
    "### Save Data to CSV\n",
    "Saves the fully merged dataset to a CSV file named `binance_crypto_data.csv` without the index column and prints a confirmation message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db33d7d8-0ad7-46e1-bf5e-6dd0533801b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/13 19:45:53 WARN Utils: Your hostname, osbdet resolves to a loopback address: 127.0.0.1; using 10.0.2.15 instead (on interface enp0s1)\n",
      "25/03/13 19:45:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/13 19:45:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to HDFS at hdfs://localhost:9000/datalake/raw/binance_crypto_data.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"SaveToHDFS\").getOrCreate()\n",
    "\n",
    "# Convert pandas DataFrame to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(data)\n",
    "\n",
    "# Save to HDFS as CSV (replace with your HDFS path)\n",
    "hdfs_path = 'hdfs://localhost:9000/datalake/raw/binance_crypto_data.csv'\n",
    "spark_df.write.csv(hdfs_path, mode='overwrite', header=True)\n",
    "\n",
    "print(f\"Data saved to HDFS at {hdfs_path}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07dec37-f0fa-430b-8fb9-3bcd5e0069d9",
   "metadata": {},
   "source": [
    "## **Step 2 - Initializing Spark Session and defining Featureas and Label**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3300e0-c74d-4eaf-8d60-0dbe86de9753",
   "metadata": {},
   "source": [
    "### Spark Imports\n",
    "Imports necessary PySpark modules for creating a Spark session, manipulating data with SQL functions (`col`, `lag`), and defining window operations (`Window`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fb53fd6-9e0b-4d91-9cff-5a2c4921a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lag\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b2b87a-140f-4b8c-83a0-49d029996281",
   "metadata": {},
   "source": [
    "### Spark Session Initialization\n",
    "Initializes a Spark session with the application name \"BTCPricePrediction\" to enable distributed data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53058230-8860-4743-81cc-2f9d67bef291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"BTCPricePrediction\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0931fec2-5fdf-48b7-a6f9-b2a4fe97fc2d",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "Loads the preprocessed cryptocurrency data from `binance_crypto_data.csv` into a Spark DataFrame, inferring the schema and using the first row as headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5064740a-f7f2-4b93-a2b2-27784639b40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|          timestamp|BTCUSDT_price|ETHUSDT_price|BNBUSDT_price|XRPUSDT_price|ADAUSDT_price|SOLUSDT_price|BTCUSDT_funding_rate|ETHUSDT_funding_rate|BNBUSDT_funding_rate|XRPUSDT_funding_rate|ADAUSDT_funding_rate|SOLUSDT_funding_rate|\n",
      "+-------------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|2024-09-08 16:00:00|     54869.95|       2297.3|        503.1|       0.5295|        0.339|       130.15|           -5.245E-5|            1.827E-5|                 0.0|            3.107E-5|            8.211E-5|            6.412E-5|\n",
      "|2024-09-09 00:00:00|     54974.01|      2314.33|        507.0|        0.531|        0.342|       129.31|           -7.585E-5|             7.67E-6|                 0.0|            1.686E-5|             5.84E-5|            3.966E-5|\n",
      "|2024-09-09 08:00:00|      55386.0|      2292.55|        506.1|       0.5294|       0.3417|       128.41|           -7.108E-5|            3.085E-5|                 0.0|            4.572E-5|              1.0E-4|           -5.564E-5|\n",
      "|2024-09-10 00:00:00|     57218.57|      2363.51|        520.2|       0.5391|       0.3423|       134.96|           -6.028E-5|            1.082E-5|             3.94E-6|            6.767E-5|              1.0E-4|            9.587E-5|\n",
      "|2024-09-10 08:00:00|      56596.0|      2325.01|        514.2|       0.5361|       0.3412|       134.14|            -2.93E-5|           -3.453E-5|                 0.0|           -1.963E-5|            7.988E-5|           -1.672E-5|\n",
      "|2024-09-10 16:00:00|     57635.99|      2388.52|        517.0|       0.5408|       0.3431|       135.63|           -1.126E-5|             2.28E-6|                 0.0|             3.57E-5|              1.0E-4|            2.349E-5|\n",
      "|2024-09-11 00:00:00|      56712.0|      2341.66|        514.1|       0.5336|       0.3399|       132.45|           -8.953E-5|            2.484E-5|                 0.0|            5.088E-5|              1.0E-4|           -6.617E-5|\n",
      "|2024-09-11 08:00:00|     56701.23|       2325.9|        518.0|       0.5321|       0.3376|       131.17|           -2.247E-5|            1.804E-5|                 0.0|            1.913E-5|            8.091E-5|           -1.813E-5|\n",
      "|2024-09-11 16:00:00|      57338.0|      2340.55|        530.1|       0.5348|       0.3532|       132.42|           -5.212E-5|           -1.796E-5|                 0.0|             9.14E-6|            3.813E-5|           -2.041E-5|\n",
      "|2024-09-12 00:00:00|     58138.16|      2359.59|        541.9|       0.5383|       0.3562|        134.9|           -1.951E-5|            2.592E-5|                 0.0|            7.149E-5|            4.783E-5|            -2.81E-5|\n",
      "|2024-09-12 08:00:00|     57512.03|       2323.2|        535.4|       0.5577|       0.3557|       133.56|            -5.46E-6|            2.496E-5|                 0.0|           -1.027E-5|            7.935E-5|             -8.6E-6|\n",
      "|2024-09-12 16:00:00|     58132.32|      2361.76|        544.5|       0.5626|       0.3564|       136.05|            -2.95E-5|            1.134E-5|                 0.0|             1.96E-6|            8.848E-5|             1.32E-5|\n",
      "|2024-09-13 00:00:00|     58109.31|      2348.88|        542.7|       0.5634|        0.357|       134.58|             -6.9E-7|            4.291E-5|                 0.0|            2.306E-5|            4.045E-5|            2.638E-5|\n",
      "|2024-09-14 00:00:00|     60138.14|      2420.23|        554.0|       0.5791|       0.3593|       137.51|           -8.699E-5|             8.45E-6|                 0.0|            -5.76E-5|           -1.986E-5|             -1.2E-6|\n",
      "|2024-09-14 08:00:00|      59876.0|      2418.54|        551.0|        0.587|       0.3546|       137.39|             8.59E-6|            -4.15E-6|                 0.0|            7.898E-5|            8.668E-5|            3.207E-5|\n",
      "|2024-09-15 00:00:00|      60196.0|       2422.9|        555.0|       0.5887|       0.3557|       137.16|            -4.91E-6|            -2.67E-6|                 0.0|            5.188E-5|             3.08E-5|            1.401E-5|\n",
      "|2024-09-15 08:00:00|     60335.41|      2408.71|        561.2|       0.5885|       0.3487|       135.76|              1.0E-8|            1.355E-5|                 0.0|            2.769E-5|            1.197E-5|             1.07E-6|\n",
      "|2024-09-15 16:00:00|      59132.0|       2316.1|        553.4|       0.5706|       0.3385|       131.37|             4.02E-6|            2.131E-5|                 0.0|             4.46E-6|            3.425E-5|            1.278E-5|\n",
      "|2024-09-16 00:00:00|      58911.1|      2310.69|        550.4|       0.5721|       0.3367|       131.25|            2.402E-5|            3.228E-5|                 0.0|            1.644E-5|            -3.46E-5|            1.892E-5|\n",
      "|2024-09-16 16:00:00|     58213.99|      2295.68|        533.7|       0.5866|       0.3303|       131.47|            6.762E-5|           -1.149E-5|           -3.343E-5|            5.446E-5|           -3.739E-5|            7.164E-5|\n",
      "+-------------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hdfs_path = 'hdfs://localhost:9000/datalake/raw/binance_crypto_data.csv'\n",
    "\n",
    "df = spark.read.csv(hdfs_path, header=True, inferSchema=True)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f3f07e-5b32-4ede-bbfc-3177a9f0dca7",
   "metadata": {},
   "source": [
    "### Define Features and Label\n",
    "Defines the feature columns (prices for non-BTC symbols and funding rates for all symbols) and the target label (`BTCUSDT_price`) for regression and classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2971cde8-b63d-4caf-aa3c-017c7e3cf8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and label\n",
    "feature_cols = [f\"{symbol}_price\" for symbol in SYMBOLS[1:]] + \\\n",
    "               [f\"{symbol}_funding_rate\" for symbol in SYMBOLS]\n",
    "label_col = \"BTCUSDT_price\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b27b492-5f91-4963-8d58-64a0148a2513",
   "metadata": {},
   "source": [
    "### Create Price Direction Label\n",
    "Uses a window function to compute the previous `BTCUSDT_price` (`prev_price`) and creates a binary `price_direction` column (1 if price increases, 0 if not). Drops rows with null values from the lag operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a9d7043-6a31-4acc-80ac-fbb92c401e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.orderBy(\"timestamp\")\n",
    "df = df.withColumn(\"prev_price\", lag(col(label_col)).over(w))\n",
    "df = df.withColumn(\"price_direction\", (col(label_col) > col(\"prev_price\")).cast(\"int\"))\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0c07ba-8ab8-4fcf-9d21-4020faad132a",
   "metadata": {},
   "source": [
    "### Assemble Features\n",
    "Imports `VectorAssembler` and combines the feature columns into a single vector column named `features` for use in machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a444fd4-c9f3-4ade-bb06-9ac2676a7480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble features into a vector\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "data = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6e8d59-352e-4301-befa-ad16c46aa8c5",
   "metadata": {},
   "source": [
    "### Select Relevant Columns and Display\n",
    "Selects the key columns (`timestamp`, `features`, `label_col`, `price_direction`) for modeling and displays the first 5 rows of the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebc69f19-0f2a-4b83-bad6-a70d5d5bb3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-------------+---------------+\n",
      "|          timestamp|            features|BTCUSDT_price|price_direction|\n",
      "+-------------------+--------------------+-------------+---------------+\n",
      "|2024-03-14 16:00:00|[3881.7,603.2,0.6...|     71388.94|              1|\n",
      "|2024-03-15 00:00:00|[3754.12,582.6,0....|     68448.29|              0|\n",
      "|2024-03-15 08:00:00|[3688.9,596.3,0.6...|     68157.07|              0|\n",
      "|2024-03-15 16:00:00|[3742.19,632.7,0....|     69499.85|              1|\n",
      "|2024-03-16 00:00:00|[3740.19,614.7,0....|     69432.45|              0|\n",
      "+-------------------+--------------------+-------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select relevant columns\n",
    "data = data.select(\"timestamp\", \"features\", label_col, \"price_direction\")\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b976e925-ab8d-4603-83bd-d3e397c4935a",
   "metadata": {},
   "source": [
    "## **Step 3 - Raw Multi Model Traininig/Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef519e8-7723-4c1f-8fbb-058b366f2f05",
   "metadata": {},
   "source": [
    "### Data Splitting\n",
    "Splits the input PySpark DataFrame into training and testing sets using an 80-20 split (80% for training, 20% for testing), with a fixed seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72727855-7dfa-46b6-bc8a-df9b573430b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (80% train, 20% test)\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f13b8c-edbf-4214-b6f1-25ca3cb22637",
   "metadata": {},
   "source": [
    "### Linear Regression Model Training\n",
    "Imports the Linear Regression class from PySpark ML, initializes it to predict a continuous price variable (e.g., `BTCUSDT_price`), and trains the model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ded297-a736-441b-ab8b-3f9b6cdad729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression (for price prediction)\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=label_col)\n",
    "lr_model = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861c6ea2-66c9-4a68-8648-d8dddd8f671d",
   "metadata": {},
   "source": [
    "### Linear Regression Evaluation\n",
    "Generates predictions on the test data using the trained Linear Regression model, evaluates them using Root Mean Squared Error (RMSE), and prints the result to assess prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a51eec-0ffc-4b6c-9fc8-d09f8ca0a96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "evaluator = RegressionEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(lr_predictions)\n",
    "print(f\"Linear Regression RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abfd5ec-ebbe-4634-a06f-a0c869b44243",
   "metadata": {},
   "source": [
    "### Logistic Regression Model Training\n",
    "Imports the Logistic Regression class from PySpark ML, initializes it to predict a binary price direction (e.g., up or down), and trains the model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4d4124-31d8-43b3-a03a-d40831c73cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression (for price direction)\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "log_reg = LogisticRegression(featuresCol=\"features\", labelCol=\"price_direction\")\n",
    "log_reg_model = log_reg.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4103e2-74a3-49db-bc93-7fe9a2d0b9dc",
   "metadata": {},
   "source": [
    "### Logistic Regression Evaluation\n",
    "Generates predictions on the test data using the trained Logistic Regression model, evaluates them using Area Under the ROC Curve (ROC AUC), and prints the result to assess classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb343ea8-9b25-4d16-b9b8-4b49ebca29ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "log_reg_predictions = log_reg_model.transform(test_data)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"price_direction\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "roc_auc = evaluator.evaluate(log_reg_predictions)\n",
    "print(f\"Logistic Regression ROC AUC: {roc_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa417b69-06ba-4081-8cad-d34f7cbad1ae",
   "metadata": {},
   "source": [
    "### Display Sample Predictions\n",
    "Selects and displays five sample rows from both Linear Regression and Logistic Regression predictions, showing timestamps, actual values, and predicted values for visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e971a1b7-7a7f-4cc9-869e-faa3f18879bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predictions.select(\"timestamp\", label_col, \"prediction\").show(5)\n",
    "log_reg_predictions.select(\"timestamp\", \"price_direction\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e24da14-577c-4fbf-bea9-af7659f469dc",
   "metadata": {},
   "source": [
    "## **Step 4 - Multi Model Traininig/Evaluation With Feature Engineering/Selection and Imbalance with SMOTE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124a65be-e40d-4c05-ba81-6986aacc97c2",
   "metadata": {},
   "source": [
    "### Imports and Setup\n",
    "Imports libraries for API requests (`requests`), data manipulation (`pandas`), visualization (`seaborn`, `matplotlib`), and PySpark ML functionalities (`SparkSession`, `functions`, `feature`, `regression`, `classification`, `evaluation`). Initializes a Spark session for data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a3756a-ed73-46a3-bace-a2b239659c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lag\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import RandomForestRegressor, LinearRegression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# Spark setup\n",
    "spark = SparkSession.builder.appName(\"BTCPricePrediction\").getOrCreate()\n",
    "df = spark.read.csv(\"binance_crypto_data.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4747fb9-7f9f-4f73-ba7e-0675ef5b37c1",
   "metadata": {},
   "source": [
    "### Feature Engineering and Label Creation\n",
    "Defines feature and label columns, creates a previous price column using a window function, and computes a binary price direction label (1 for up, 0 for down) based on price changes, dropping rows with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38f6de5-77fe-4e57-a49d-e37358421f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and labels\n",
    "feature_cols = [f\"{symbol}_price\" for symbol in SYMBOLS[1:]] + [f\"{symbol}_funding_rate\" for symbol in SYMBOLS]\n",
    "label_col = \"BTCUSDT_price\"\n",
    "w = Window.orderBy(\"timestamp\")\n",
    "df = df.withColumn(\"prev_price\", lag(col(label_col)).over(w))\n",
    "df = df.withColumn(\"price_direction\", (col(label_col) > col(\"prev_price\")).cast(\"int\")).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d7c733-f762-4d57-bebc-25f536acb9ab",
   "metadata": {},
   "source": [
    "### Correlation Heatmap Visualization\n",
    "Converts the PySpark DataFrame to Pandas, computes a correlation matrix for features and the label, and visualizes it as a heatmap using Seaborn to identify relationships between variables, saving the plot as a PNG file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b568c4c1-6c1c-4a54-bae1-5e8dd7ba872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Heatmap\n",
    "pandas_df = df.select(feature_cols + [label_col]).toPandas()\n",
    "corr_matrix = pandas_df.corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.savefig(\"correlation_heatmap.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90186491-df72-498a-9a8a-f0f47de4f58e",
   "metadata": {},
   "source": [
    "### Feature Selection with Random Forest\n",
    "Assembles all features into a vector, trains a Random Forest Regressor to determine feature importances, and selects the top 8 most important features for modeling, printing the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a59cbd-693e-44aa-9564-6b8735a4ac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection with Random Forest\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "data = assembler.transform(df)\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=label_col, numTrees=20, seed=42)\n",
    "rf_model = rf.fit(data)\n",
    "importances = rf_model.featureImportances\n",
    "feature_importance = sorted(zip(feature_cols, importances), key=lambda x: x[1], reverse=True)\n",
    "selected_cols = [f[0] for f in feature_importance[:8]]\n",
    "print(f\"Selected Features: {selected_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98726e90-855a-48d0-8e15-797900c7bd6e",
   "metadata": {},
   "source": [
    "### Feature Assembly and Scaling\n",
    "Assembles the selected features into a vector and scales them using StandardScaler to standardize the data (mean=0, std=1) for better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de308df-c5b2-4a0b-bc6d-d34ef166e4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble selected features\n",
    "assembler_selected = VectorAssembler(inputCols=selected_cols, outputCol=\"raw_features\")\n",
    "data_selected = assembler_selected.transform(df)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"features\", withStd=True, withMean=True)\n",
    "scaler_model = scaler.fit(data_selected)\n",
    "data_scaled = scaler_model.transform(data_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c88153-1f7a-43a3-8af8-d78eaec10e51",
   "metadata": {},
   "source": [
    "### Class Imbalance Check and SMOTE Oversampling\n",
    "Checks the distribution of `price_direction` classes (0 and 1), and if imbalanced (ratio > 1.5), applies a SMOTE-like oversampling by replicating the minority class to balance the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407433b6-3df6-4236-9683-8a9b62ab2c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check imbalance and apply SMOTE if needed\n",
    "counts = data_scaled.groupBy(\"price_direction\").count().collect()\n",
    "majority_count = next(row[\"count\"] for row in counts if row[\"price_direction\"] == 0)\n",
    "minority_count = next(row[\"count\"] for row in counts if row[\"price_direction\"] == 1)\n",
    "print(f\"Class counts: 0={majority_count}, 1={minority_count}\")\n",
    "if majority_count / minority_count > 1.5:\n",
    "    majority = data_scaled.filter(col(\"price_direction\") == 0)\n",
    "    minority = data_scaled.filter(col(\"price_direction\") == 1)\n",
    "    ratio = majority_count / minority_count\n",
    "    oversampled_minority = minority.sample(withReplacement=True, fraction=ratio, seed=42)\n",
    "    balanced_data = majority.union(oversampled_minority)\n",
    "    print(\"Applied SMOTE-like oversampling.\")\n",
    "else:\n",
    "    balanced_data = data_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963bc065-f691-43dd-98ef-e3e894425661",
   "metadata": {},
   "source": [
    "### Data Splitting\n",
    "Splits the balanced dataset into training (80%) and testing (20%) sets with a fixed seed for reproducibility, preparing it for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771d23d0-1ebe-4179-8774-f8e15bfacebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_data, test_data = balanced_data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc73d25a-721e-40a5-879f-e27acb937e1b",
   "metadata": {},
   "source": [
    "### Linear Regression Training and Evaluation\n",
    "Trains a Linear Regression model to predict `BTCUSDT_price`, generates predictions on the test set, and evaluates them using RMSE, printing the result to assess performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da98fed3-f04c-4b50-930e-6b72144ccb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=label_col)\n",
    "lr_model = lr.fit(train_data)\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "evaluator = RegressionEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(lr_predictions)\n",
    "print(f\"Linear Regression RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03001da-f7dd-4327-9afd-fc8d8ad65ffa",
   "metadata": {},
   "source": [
    "### Logistic Regression Training and Evaluation\n",
    "Trains a Logistic Regression model to predict `price_direction`, generates predictions on the test set, and evaluates them using ROC AUC, printing the result to assess classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c300ce-4771-4921-9034-96852755ad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(featuresCol=\"features\", labelCol=\"price_direction\")\n",
    "log_reg_model = log_reg.fit(train_data)\n",
    "log_reg_predictions = log_reg_model.transform(test_data)\n",
    "roc_evaluator = BinaryClassificationEvaluator(labelCol=\"price_direction\", metricName=\"areaUnderROC\")\n",
    "roc_auc = roc_evaluator.evaluate(log_reg_predictions)\n",
    "print(f\"Logistic Regression ROC AUC: {roc_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f03db1f-e3a2-41c7-8d67-3298298950c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.write().overwrite().save(\"./models/lr_model\") \n",
    "log_reg_model.write().overwrite().save(\"./models/log_reg_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5056fda-9808-4c6d-a5db-fdc4c6072694",
   "metadata": {},
   "source": [
    "### Confusion Matrix Visualization\n",
    "Computes a confusion matrix from Logistic Regression predictions, converts it to a Pandas DataFrame, and visualizes it as a heatmap using Seaborn to show true vs. predicted classifications, saving the plot as a PNG file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157f4b4c-e523-435e-a7d4-57a60ae4e8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix = log_reg_predictions.groupBy(\"price_direction\", \"prediction\").count().toPandas()\n",
    "conf_matrix_pivot = conf_matrix.pivot(index=\"price_direction\", columns=\"prediction\", values=\"count\").fillna(0)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix_pivot, annot=True, fmt=\"g\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70665693-bfb6-4844-8317-bec6f8392764",
   "metadata": {},
   "source": [
    "### Visualize Linear Regression Predictions\n",
    "Checks the available columns in the Linear Regression predictions DataFrame, converts it to a Pandas DataFrame, and creates a line plot comparing actual and predicted BTC prices over time using Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7394b2b4-5fa1-42d3-92b2-0437feddb1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the prediction DataFrame to Pandas\n",
    "lr_predictions_df = lr_predictions.select(\"timestamp\", \"BTCUSDT_price\", \"prediction\").toPandas()\n",
    "\n",
    "# Rename for clarity\n",
    "lr_predictions_df.rename(columns={\"prediction\": \"predicted_price\"}, inplace=True)\n",
    "\n",
    "# Sort by timestamp\n",
    "lr_predictions_df = lr_predictions_df.sort_values(by=\"timestamp\")\n",
    "\n",
    "# Plotting actual vs predicted BTC prices\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(lr_predictions_df[\"timestamp\"], lr_predictions_df[\"BTCUSDT_price\"], label=\"Actual BTC Price\", marker='o')\n",
    "plt.plot(lr_predictions_df[\"timestamp\"], lr_predictions_df[\"predicted_price\"], label=\"Predicted BTC Price\", marker='x')\n",
    "plt.xlabel(\"Timestamp\")\n",
    "plt.ylabel(\"BTC Price (USDT)\")\n",
    "plt.title(\"Actual vs Predicted BTC Prices Over Time\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e3bf9a-35c8-4078-ba48-681ebf589e98",
   "metadata": {},
   "source": [
    "## **Step 5 - Simulated Streaming Data Forecasting with MicroBatching**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be012e6c-f4eb-4cce-be69-d6203e47597c",
   "metadata": {},
   "source": [
    "### Bitcoin Price Prediction Streaming Pipeline\n",
    "This script implements a real-time Bitcoin price prediction pipeline using PySpark streaming, fetching cryptocurrency data from Binance APIs, including prices and funding rates for symbols like BTCUSDT and ETHUSDT. \n",
    "\n",
    "Initializing a Spark session with optimized configurations, defining a schema for structured streaming data, simulating a streaming source by writing JSON files to a directory.\n",
    "\n",
    "Following, the processing the stream with watermarking and windowed aggregations, preparing the data for machine learning by assembling features, applying pre-trained Linear and Logistic Regression models to predict price and direction, and managing stream execution with proper resource cleanup upon completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b2f8b4-4c99-41d2-9770-2b314b537879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import threading\n",
    "import shutil\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, window, max as max_, expr, avg\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "\n",
    "# Constants and setup\n",
    "BASE_URL = \"https://api.binance.com\"\n",
    "FUTURES_URL = \"https://fapi.binance.com\"\n",
    "SYMBOLS = [\"BTCUSDT\", \"ETHUSDT\", \"BNBUSDT\", \"XRPUSDT\", \"ADAUSDT\", \"SOLUSDT\"]\n",
    "\n",
    "spark = SparkSession.builder.appName(\"BTCPricePredictionStreaming\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", \"true\") \\\n",
    "    .config(\"spark.sql.streaming.statefulOperator.allowMultiple\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Define schema with proper types\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "] + [\n",
    "    StructField(f\"{s}_price\", FloatType(), True) for s in SYMBOLS\n",
    "] + [\n",
    "    StructField(f\"{s}_funding_rate\", FloatType(), True) for s in SYMBOLS\n",
    "])\n",
    "\n",
    "def fetch_realtime_data():\n",
    "    data = {\"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())}\n",
    "    for s in SYMBOLS:\n",
    "        try:\n",
    "            price_url = f\"{BASE_URL}/api/v3/ticker/price?symbol={s}\"\n",
    "            funding_url = f\"{FUTURES_URL}/fapi/v1/premiumIndex?symbol={s}\"\n",
    "            price_resp = requests.get(price_url, timeout=5)\n",
    "            funding_resp = requests.get(funding_url, timeout=5)\n",
    "            price_resp.raise_for_status()\n",
    "            funding_resp.raise_for_status()\n",
    "            data[f\"{s}_price\"] = float(price_resp.json()[\"price\"])\n",
    "            data[f\"{s}_funding_rate\"] = float(funding_resp.json()[\"lastFundingRate\"])\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching {s}: {e}\")\n",
    "            data[f\"{s}_price\"] = 0.0  # default\n",
    "            data[f\"{s}_funding_rate\"] = 0.0\n",
    "    print(f\"Generated data: {data}\")\n",
    "    return json.dumps(data)\n",
    "\n",
    "# Simulate streaming source\n",
    "stream_dir = \"streaming_input\"\n",
    "if os.path.exists(stream_dir):\n",
    "    shutil.rmtree(stream_dir)  # clean the directory before starting\n",
    "os.makedirs(stream_dir, exist_ok=True)\n",
    "stop_event = threading.Event()\n",
    "\n",
    "# Write initial data\n",
    "base_time = int(time.time())\n",
    "for i in range(3):\n",
    "    filename = f\"{stream_dir}/init_data_{base_time - 20 + i*10}.json\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(fetch_realtime_data())\n",
    "    print(f\"Wrote initial {filename}\")\n",
    "    time.sleep(1)\n",
    "\n",
    "def write_stream_data():\n",
    "    while not stop_event.is_set():\n",
    "        filename = f\"{stream_dir}/data_{int(time.time())}.json\"\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(fetch_realtime_data())\n",
    "        time.sleep(5)\n",
    "\n",
    "stream_thread = threading.Thread(target=write_stream_data, daemon=True)\n",
    "stream_thread.start()\n",
    "\n",
    "try:\n",
    "    # Read stream and convert timestamp string to timestamp\n",
    "    streaming_df = spark.readStream.schema(schema).json(stream_dir)\n",
    "    streaming_df = streaming_df.withColumn(\"timestamp\", expr(\"cast(timestamp as timestamp)\"))\n",
    "    \n",
    "    # Debug raw streaming data\n",
    "    raw_query = streaming_df.writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"console\") \\\n",
    "        .option(\"truncate\", \"false\") \\\n",
    "        .trigger(processingTime=\"5 seconds\") \\\n",
    "        .start()\n",
    "    \n",
    "    # Apply watermark\n",
    "    watermarked_df = streaming_df.withWatermark(\"timestamp\", \"2 minutes\")\n",
    "    \n",
    "    # Use a sliding window: window duration = 30 seconds, slide duration = 10 seconds\n",
    "    windowed_df = watermarked_df.groupBy(window(col(\"timestamp\"), \"30 seconds\", \"10 seconds\").alias(\"time_window\")) \\\n",
    "                              .agg(avg(col(\"BTCUSDT_price\")).alias(\"BTCUSDT_price\"),\n",
    "                                   *[avg(col(f\"{s}_price\")).alias(f\"{s}_price\") for s in SYMBOLS[1:]],\n",
    "                                   *[avg(col(f\"{s}_funding_rate\")).alias(f\"{s}_funding_rate\") for s in SYMBOLS])\n",
    "    \n",
    "    # Debug windowed data\n",
    "    windowed_query = windowed_df.writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"console\") \\\n",
    "        .option(\"truncate\", \"false\") \\\n",
    "        .trigger(processingTime=\"5 seconds\") \\\n",
    "        .start()\n",
    "    \n",
    "    # Prepare data for model\n",
    "    windowed_df_with_ts = windowed_df.select(\n",
    "        col(\"time_window.start\").alias(\"window_start\"),\n",
    "        col(\"time_window.end\").alias(\"window_end\"),\n",
    "        col(\"BTCUSDT_price\"),\n",
    "        *[col(f\"{s}_price\") for s in SYMBOLS[1:]],\n",
    "        *[col(f\"{s}_funding_rate\") for s in SYMBOLS]\n",
    "    )\n",
    "    \n",
    "    model_dir = \"/home/osbdet/notebooks/mda2/group_project/models\"\n",
    "    \n",
    "    def process_batch(batch_df, batch_id):\n",
    "        if batch_df.count() > 0:\n",
    "            try:\n",
    "                print(f\"Processing batch {batch_id} with {batch_df.count()} records\")\n",
    "                \n",
    "                # Hard-coded feature list matching training\n",
    "                selected_cols = [\n",
    "                    \"ETHUSDT_price\",\n",
    "                    \"BNBUSDT_price\",\n",
    "                    \"XRPUSDT_price\",\n",
    "                    \"ADAUSDT_price\",\n",
    "                    \"SOLUSDT_price\",\n",
    "                    \"BTCUSDT_funding_rate\",\n",
    "                    \"ETHUSDT_funding_rate\",\n",
    "                    \"BNBUSDT_funding_rate\"\n",
    "                ]\n",
    "                print(f\"Using features: {selected_cols}\")\n",
    "                \n",
    "                # Assemble features\n",
    "                assembler = VectorAssembler(inputCols=selected_cols, outputCol=\"raw_features\")\n",
    "                assembled_df = assembler.transform(batch_df)\n",
    "                print(\"Assembled raw features:\")\n",
    "                assembled_df.select(\"raw_features\").show(truncate=False)\n",
    "                \n",
    "                # Check record count  if only one record, bypass scaling\n",
    "                record_count = assembled_df.count()\n",
    "                if record_count > 1:\n",
    "                    scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"features\",\n",
    "                                            withStd=True, withMean=True)\n",
    "                    scaler_model = scaler.fit(assembled_df)\n",
    "                    scaled_df = scaler_model.transform(assembled_df)\n",
    "                else:\n",
    "                    print(\"Only one record in batch; using raw_features directly as features.\")\n",
    "                    scaled_df = assembled_df.withColumn(\"features\", col(\"raw_features\"))\n",
    "                \n",
    "                print(\"Scaled features:\")\n",
    "                scaled_df.select(\"features\").show(truncate=False)\n",
    "                # Load and apply models\n",
    "\n",
    "                # Define window for price direction calculation\n",
    "                w = Window.orderBy(\"window_start\")\n",
    "                \n",
    "                # Add prev_price and compute price_direction\n",
    "                scaled_df = scaled_df.withColumn(\"prev_price\", lag(col(\"BTCUSDT_price\")).over(w)) \\\n",
    "                                    .withColumn(\"price_direction\", \n",
    "                                                expr(\"CASE WHEN BTCUSDT_price > prev_price THEN 1 ELSE 0 END\")) \\\n",
    "                                    .dropna()  # Drop rows where prev_price is null\n",
    "                \n",
    "                if os.path.exists(f\"{model_dir}/lr_model\") and os.path.exists(f\"{model_dir}/log_reg_model\"):\n",
    "                    # Load Linear Regression model\n",
    "                    lr_model = LinearRegressionModel.load(f\"{model_dir}/lr_model\")\n",
    "                    lr_predicted_df = lr_model.transform(scaled_df) \\\n",
    "                                            .withColumnRenamed(\"prediction\", \"predicted_price\")  # Rename LR prediction\n",
    "                    \n",
    "                    # Load Logistic Regression model\n",
    "                    log_reg_model = LogisticRegressionModel.load(f\"{model_dir}/log_reg_model\")\n",
    "                    final_predicted_df = log_reg_model.transform(lr_predicted_df) \\\n",
    "                        .withColumnRenamed(\"BTCUSDT_price\", \"actual_price\") \\\n",
    "                        .withColumnRenamed(\"prev_price\", \"previous_price\")\n",
    "                    \n",
    "                    # Show results\n",
    "                    print(f\"Batch {batch_id} Results:\")\n",
    "                    final_predicted_df.select(\n",
    "                        \"window_start\",\n",
    "                        \"actual_price\",\n",
    "                        \"predicted_price\",  # LR prediction\n",
    "                        \"price_direction\",\n",
    "                        \"prediction\"  # LogReg prediction as predicted_direction\n",
    "                    ).show(truncate=False)\n",
    "                else:\n",
    "                    missing_models = []\n",
    "                    if not os.path.exists(f\"{model_dir}/lr_model\"):\n",
    "                        missing_models.append(\"lr_model\")\n",
    "                    if not os.path.exists(f\"{model_dir}/log_reg_model\"):\n",
    "                        missing_models.append(\"log_reg_model\")\n",
    "                    print(f\"Models not found: {missing_models}\")\n",
    "                    scaled_df.select(\"window_start\", \"BTCUSDT_price\").show(truncate=False)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch {batch_id}: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        else:\n",
    "            print(f\"Empty batch {batch_id}\")\n",
    "    \n",
    "    # Process streaming data with foreachBatch\n",
    "    prediction_query = windowed_df_with_ts.writeStream \\\n",
    "        .foreachBatch(process_batch) \\\n",
    "        .outputMode(\"update\") \\\n",
    "        .trigger(processingTime=\"10 seconds\") \\\n",
    "        .start()\n",
    "    \n",
    "    spark.streams.awaitAnyTermination()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in main processing: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "finally:\n",
    "    print(\"Stopping streams and cleaning up...\")\n",
    "    stop_event.set()\n",
    "    \n",
    "    for query_name in ['raw_query', 'windowed_query', 'prediction_query']:\n",
    "        if query_name in locals() and locals()[query_name] is not None:\n",
    "            try:\n",
    "                locals()[query_name].stop()\n",
    "                print(f\"Stopped {query_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error stopping {query_name}: {e}\")\n",
    "    \n",
    "    time.sleep(2)\n",
    "    if os.path.exists(stream_dir):\n",
    "        shutil.rmtree(stream_dir)\n",
    "    \n",
    "    spark.stop()\n",
    "    print(\"Streaming stopped and resources cleaned up.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
